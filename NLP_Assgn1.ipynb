{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "Sc9hkyQN0710"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "from typing import Iterator\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRdicvUs4liT",
        "outputId": "82fadd26-f222-4075-8f6f-d9e685a34a9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: Unsmoothed Unigrams and Bigrams"
      ],
      "metadata": {
        "id": "yeIoFPq6cPUc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "zRqom_Nl4szm"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(sentence):\n",
        "    remove_punctuation = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    word_tokens = word_tokenize(remove_punctuation)\n",
        "    clean = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for word in word_tokens:\n",
        "        clean.append(word)\n",
        "    return ' '.join(w for w in clean)\n",
        "\n",
        "def n_grams(s,n,i=0):\n",
        "    while(len(s[i:i+n])==n):\n",
        "        yield s[i:i+n]\n",
        "        i+=1\n",
        "\n",
        "def unigram_probabilities(unigram, word_frequency_dict, total_vocab):\n",
        "    unigram_prob={}\n",
        "    for sentence in unigram:\n",
        "        for word in sentence:\n",
        "            unigram_prob[word[0]]=word_frequency_dict[word[0]]/total_vocab\n",
        "    return unigram_prob\n",
        "\n",
        "\n",
        "def bigram_probabilites(bigram, word_frequency_dict, bigram_dict):\n",
        "    bigram_prob={}\n",
        "    for sentence in bigram:\n",
        "        for pair in sentence:\n",
        "          count_pair=bigram_dict[(pair[0],pair[1])]\n",
        "          if(pair[0] in word_frequency_dict):\n",
        "              bigram_prob[(pair[0],pair[1])]=count_pair/word_frequency_dict[pair[0]]\n",
        "          else:\n",
        "              bigram_prob[(pair[0],pair[1])] = 0\n",
        "    return bigram_prob\n",
        "\n",
        "with open('train.txt', 'r', encoding='utf-8') as file:\n",
        "    corpus = file.read().lower()\n",
        "sentences = corpus.split(\"\\n\")\n",
        "bigrams = []\n",
        "unigrams=[]\n",
        "for sentence in sentences:\n",
        "    preprocessed_sentence = preprocess_text(sentence)\n",
        "    bigrams.append(list(n_grams(preprocessed_sentence.split(), 2)))\n",
        "    unigrams.append(list(n_grams(preprocessed_sentence.split(), 1)))\n",
        "\n",
        "word_frequency_dict = Counter(corpus.split())\n",
        "total_vocab=0\n",
        "unique_unigrams = set({})\n",
        "for sentence in unigrams:\n",
        "    for word in sentence:\n",
        "        unique_unigrams.add(word[0])\n",
        "for word in unique_unigrams:\n",
        "    total_vocab += word_frequency_dict[word]\n",
        "bigram_dict={}\n",
        "for sentence in bigrams:\n",
        "    for pair in sentence:\n",
        "        if((pair[0],pair[1]) in bigram_dict):\n",
        "            bigram_dict[(pair[0],pair[1])]+=1\n",
        "        else:\n",
        "            bigram_dict[(pair[0],pair[1])]=1\n",
        "\n",
        "unigram_prob=unigram_probabilities(unigrams,word_frequency_dict, total_vocab)\n",
        "bigram_prob=bigram_probabilites(bigrams,word_frequency_dict, bigram_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Handling unknown words"
      ],
      "metadata": {
        "id": "CvgbHUK2cWQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_word_freq={k: v for k, v in sorted(word_frequency_dict.items(), key=lambda item: item[1], reverse = True)}\n",
        "count=0\n",
        "unknown_words_train=[]\n",
        "new_sorted_dict={}\n",
        "for key,value in sorted_word_freq.items():\n",
        "    if key in unique_unigrams:\n",
        "        if value<2:\n",
        "            count+=1\n",
        "            unknown_words_train.append(key)\n",
        "        else:\n",
        "            new_sorted_dict[key]=value\n",
        "\n",
        "new_sorted_dict['<UNK>']=count\n",
        "\n",
        "#calculate probabilities after handling unknown words\n",
        "def new_unigram_probabilities(unigram, word_frequency_dict, total_vocab, new_sorted_dict):\n",
        "    unigram_prob={}\n",
        "    for sentence in unigram:\n",
        "        for word in sentence:\n",
        "            if word[0] not in unknown_words_train:\n",
        "                if(word[0] in word_frequency_dict):\n",
        "                    unigram_prob[word[0]] = word_frequency_dict[word[0]]/total_vocab\n",
        "                else:\n",
        "                    unigram_prob[word[0]] = 0\n",
        "            else:\n",
        "                unigram_prob[word[0]]=new_sorted_dict['<UNK>']/total_vocab\n",
        "    return unigram_prob\n",
        "\n",
        "new_unigram_prob= new_unigram_probabilities(unigrams,new_sorted_dict,total_vocab,new_sorted_dict)\n",
        "\n",
        "#smoothing after handling unknown words and re-calulating probabilities\n",
        "def new_smooth_unigram_probabilities(unigram, word_frequency_dict, total_vocab, new_sorted_dict,smoothing_factor):\n",
        "    unigram_prob={}\n",
        "    for sentence in unigram:\n",
        "        for word in sentence:\n",
        "            if word[0] not in unknown_words_train:\n",
        "                if(word[0] in word_frequency_dict):\n",
        "                    unigram_prob[word[0]]=(word_frequency_dict[word[0]]+smoothing_factor)/(total_vocab*(smoothing_factor+1))\n",
        "                else:\n",
        "                    unigram_prob[word[0]]=(smoothing_factor)/(total_vocab*(smoothing_factor+1))\n",
        "            else:\n",
        "                unigram_prob[word[0]]=(new_sorted_dict['<UNK>']+smoothing_factor)/(total_vocab*(smoothing_factor+1))\n",
        "    return unigram_prob\n",
        "\n",
        "smooth_unigram_prob= new_smooth_unigram_probabilities(unigrams,new_sorted_dict,total_vocab,new_sorted_dict,5) # hyper parameter is set for unigrams\n"
      ],
      "metadata": {
        "id": "OSei_OYpQZx7"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing the words with frequency 1 in the corpus with ```'<UNK>'``` and creating a new file 'new_train.txt'"
      ],
      "metadata": {
        "id": "LHLkz35ZbfSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create new text file with unknown words so that bigram calculations become easier\n",
        "preprocessed_updated_content = []\n",
        "with open('train.txt', 'r', encoding='utf-8') as file:\n",
        "    new_file=file.read()\n",
        "    sentences = new_file.split(\"\\n\")\n",
        "    bigrams = []\n",
        "    for sentence in sentences:\n",
        "        preprocessed_sentence = preprocess_text(sentence)\n",
        "        bigrams.append(list(n_grams(preprocessed_sentence.split(), 2)))\n",
        "        for word in unknown_words_train:\n",
        "            preprocessed_sentence.replace(word,'<UNK>')\n",
        "        preprocessed_updated_content.append(preprocessed_sentence)\n",
        "\n",
        "with open('new_train.txt', 'w', encoding='utf-8') as file:\n",
        "    for new_sentence in preprocessed_updated_content:\n",
        "        file.write(new_sentence)\n",
        "        file.write(\"\\n\")"
      ],
      "metadata": {
        "id": "5zayHByZPAfT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating bigram probabilites after unknown handling"
      ],
      "metadata": {
        "id": "S8ZqEYNZbcXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('new_train.txt','r',encoding='utf-8') as file:\n",
        "    new_corpus=file.read()\n",
        "#creating bigrams for each review\n",
        "train_sentences=new_corpus.split('\\n')\n",
        "\n",
        "new_bigrams= n_grams(new_corpus.split(),2)\n",
        "new_bigrams = [(pair[0],pair[1]) for pair in new_bigrams]\n",
        "new_bigram_dict={}\n",
        "for pair in new_bigrams:\n",
        "    if((pair[0],pair[1]) in new_bigram_dict):\n",
        "        new_bigram_dict[(pair[0],pair[1])]+=1\n",
        "    else:\n",
        "        new_bigram_dict[(pair[0],pair[1])]=1\n",
        "\n",
        "new_bigram_prob={}\n",
        "for pair in new_bigrams:\n",
        "    count_pair=new_bigram_dict[pair]\n",
        "    if (pair[0] in new_sorted_dict):\n",
        "        new_bigram_prob[pair]=(count_pair)/(new_sorted_dict[pair[0]])\n",
        "    else:\n",
        "        new_bigram_prob[pair]=(count_pair)/(new_sorted_dict['<UNK>'])\n"
      ],
      "metadata": {
        "id": "5babrdxFy6nR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smoothed bigram probabilites"
      ],
      "metadata": {
        "id": "4kJ_IAx80WaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smooth_bigram_prob={}\n",
        "factor = 5 #hyper-parameter\n",
        "for pair in new_bigrams:\n",
        "    count_pair=new_bigram_dict[pair]\n",
        "    if (pair[0] in new_sorted_dict):\n",
        "        smooth_bigram_prob[pair]=(count_pair+factor)/(new_sorted_dict[pair[0]]+factor*total_vocab)\n",
        "    else:\n",
        "        smooth_bigram_prob[pair]=(count_pair+factor)/(new_sorted_dict['<UNK>']+factor*total_vocab)"
      ],
      "metadata": {
        "id": "5DeyQ1uq0VGy"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "total_vocab_val=0\n",
        "with open('val.txt', 'r', encoding='utf-8') as file:\n",
        "    valtxt = str(file.read().lower())\n",
        "valtxt = preprocess_text(valtxt)\n",
        "word_frequency_dict_val = Counter(valtxt.split())\n",
        "total_vocab_val=sum(word_frequency_dict_val.values())\n",
        "unique_vocab=len(word_frequency_dict_val)"
      ],
      "metadata": {
        "id": "QrI6EcjO1mYY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unigram Perplexity"
      ],
      "metadata": {
        "id": "bUgDGinjrDhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_log_unigram = 0\n",
        "sentence_unigram = n_grams(valtxt.split(), 1)\n",
        "log_sum_unigram = 0\n",
        "for word in sentence_unigram:\n",
        "    if word[0] in smooth_unigram_prob:\n",
        "        if unigram_prob[word[0]]!=0:\n",
        "            log_sum_unigram+=math.log(smooth_unigram_prob[word[0]])\n",
        "        else:\n",
        "            log_sum_unigram+=math.log(count/total_vocab)\n",
        "    else:\n",
        "        log_sum_unigram+=math.log(count/total_vocab)\n",
        "total_log_unigram += log_sum_unigram\n",
        "\n",
        "total_log_unigram *= -1/total_vocab_val\n",
        "total_perplexity_unigram = math.exp(total_log_unigram)\n",
        "print(\"Perplexity:\", total_perplexity_unigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlt5lGXzrCaU",
        "outputId": "4c1d2b35-497a-47b3-c443-b27f781aec26"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 1624.0819671836339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram Perplexity"
      ],
      "metadata": {
        "id": "NeT56pJorfje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_perplexity = 0\n",
        "total_log = 0\n",
        "sentence_bigram = n_grams(valtxt.split(), 2)\n",
        "log_sum = 0\n",
        "for pairs in sentence_bigram:\n",
        "  if tuple(pairs) in smooth_bigram_prob:\n",
        "      if bigram_prob[tuple(pairs)]!=0:\n",
        "          log_sum+=math.log(smooth_bigram_prob[tuple(pairs)])\n",
        "      else:\n",
        "          log_sum+=math.log(count/total_vocab)\n",
        "  else:\n",
        "      log_sum+=math.log(count/total_vocab)\n",
        "total_log += log_sum\n",
        "\n",
        "total_log *= -1/total_vocab_val\n",
        "total_perplexity = math.exp(total_log)\n",
        "print(\"Perplexity:\", total_perplexity)"
      ],
      "metadata": {
        "id": "_e-PnAmnb2h2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43dc810-48b1-43ad-824c-64a83d6d39ac"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 1418.5965695615337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "giG8IC5U2B6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}